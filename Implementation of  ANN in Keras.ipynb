{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fbe9b03-2b3a-40e8-9e47-081e907017f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\pc\\anaconda3\\lib\\site-packages (2.17.0)\n",
      "Requirement already satisfied: keras in c:\\users\\pc\\anaconda3\\lib\\site-packages (3.5.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.32.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.66.2)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.17.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: rich in c:\\users\\pc\\anaconda3\\lib\\site-packages (from keras) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\pc\\anaconda3\\lib\\site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\pc\\anaconda3\\lib\\site-packages (from keras) (0.12.1)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from rich->keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\pc\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras) (0.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2024.7.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.1.3)\n",
      "TensorFlow version: 2.17.0\n",
      "Keras version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "## Q1. Install and load the latest versions of TensorFlow and Keras. Print their versions.\n",
    "\n",
    "# Install TensorFlow and Keras\n",
    "!pip install tensorflow keras\n",
    "\n",
    "# Load the libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Print their versions\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Keras version:\", keras.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dab2c256-446c-4e3c-8d99-e3ff807c67d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.17.0\n",
      "Keras version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Print the versions\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d01e4244-6bf5-45fb-b880-be75ce54ed57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.17.0\n",
      "Keras version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "# If you haven't installed TensorFlow yet, uncomment the following line:\n",
    "# !pip install tensorflow\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Print the versions\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7a6cd78-0dc5-4c36-9c25-0873cb39f2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q3. Check for null values, identify categorical variables, and encode them.\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "# Load the dataset\n",
    "df = sns.load_dataset('titanic')  # Replace 'titanic.csv' with your dataset file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5211296e-a63a-4723-a0c6-2aa832627e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Values in Each Column:\n",
      " survived         0\n",
      "pclass           0\n",
      "sex              0\n",
      "age            177\n",
      "sibsp            0\n",
      "parch            0\n",
      "fare             0\n",
      "embarked         2\n",
      "class            0\n",
      "who              0\n",
      "adult_male       0\n",
      "deck           688\n",
      "embark_town      2\n",
      "alive            0\n",
      "alone            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for null values\n",
    "null_values = df.isnull().sum()\n",
    "print(\"Null Values in Each Column:\\n\", null_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d6f6875-89d1-484e-a365-983a47785ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Categorical Variables:\n",
      " Index(['sex', 'embarked', 'class', 'who', 'deck', 'embark_town', 'alive'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Identify categorical variables\n",
    "categorical_vars = df.select_dtypes(include=['object', 'category']).columns\n",
    "print(\"\\nCategorical Variables:\\n\", categorical_vars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c2fc16d-aefb-4feb-a3bf-8af40bce9f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data after One-Hot Encoding:\n",
      "    survived  pclass   age  sibsp  parch     fare  adult_male  alone  sex_male  \\\n",
      "0         0       3  22.0      1      0   7.2500        True  False      True   \n",
      "1         1       1  38.0      1      0  71.2833       False  False     False   \n",
      "2         1       3  26.0      0      0   7.9250       False   True     False   \n",
      "3         1       1  35.0      1      0  53.1000       False  False     False   \n",
      "4         0       3  35.0      0      0   8.0500        True   True      True   \n",
      "\n",
      "   embarked_Q  ...  who_woman  deck_B  deck_C  deck_D  deck_E  deck_F  deck_G  \\\n",
      "0       False  ...      False   False   False   False   False   False   False   \n",
      "1       False  ...       True   False    True   False   False   False   False   \n",
      "2       False  ...       True   False   False   False   False   False   False   \n",
      "3       False  ...       True   False    True   False   False   False   False   \n",
      "4       False  ...      False   False   False   False   False   False   False   \n",
      "\n",
      "   embark_town_Queenstown  embark_town_Southampton  alive_yes  \n",
      "0                   False                     True      False  \n",
      "1                   False                    False       True  \n",
      "2                   False                     True       True  \n",
      "3                   False                     True       True  \n",
      "4                   False                     True      False  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "# One-Hot Encoding\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_vars, drop_first=True)\n",
    "print(\"\\nData after One-Hot Encoding:\\n\", df_encoded.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "150b6fa9-28c6-4c13-b461-0121c0f7d06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create a LabelEncoder object\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Example: encoding 'Embarked' variable if it's ordinal\n",
    "if 'Embarked' in categorical_vars:\n",
    "    df['Embarked'] = label_encoder.fit_transform(df['Embarked'])\n",
    "    print(\"\\nData after Label Encoding for 'Embarked':\\n\", df[['Embarked']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91f54e7c-7cbf-439b-a0e6-e2836723ece7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:\n",
      "    feature1  feature2\n",
      "0         1         4\n",
      "1         2         5\n",
      "2         3         6\n",
      "Target:\n",
      " 0    0\n",
      "1    1\n",
      "2    0\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Q4. Separate the features and target variables from the dataset.\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    'feature1': [1, 2, 3],\n",
    "    'feature2': [4, 5, 6],\n",
    "    'target': [0, 1, 0]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Separate features and target\n",
    "X = df[['feature1', 'feature2']]  # Features\n",
    "y = df['target']                   # Target\n",
    "\n",
    "print(\"Features:\\n\", X)\n",
    "print(\"Target:\\n\", y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8be114ba-010a-4a0d-be64-8429f88e2cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:\n",
      " [[1 4]\n",
      " [2 5]\n",
      " [3 6]]\n",
      "Target:\n",
      " [0 1 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample dataset\n",
    "data = np.array([[1, 4, 0],\n",
    "                 [2, 5, 1],\n",
    "                 [3, 6, 0]])\n",
    "\n",
    "# Separate features and target\n",
    "X = data[:, :-1]  # All rows, all columns except the last\n",
    "y = data[:, -1]   # All rows, only the last column\n",
    "\n",
    "print(\"Features:\\n\", X)\n",
    "print(\"Target:\\n\", y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e132c879-975b-4123-aefa-d6ef00696689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features:\n",
      "    feature1  feature2\n",
      "1         2         5\n",
      "2         3         6\n",
      "Testing Features:\n",
      "    feature1  feature2\n",
      "0         1         4\n",
      "Training Target:\n",
      " 1    1\n",
      "2    0\n",
      "Name: target, dtype: int64\n",
      "Testing Target:\n",
      " 0    0\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    'feature1': [1, 2, 3],\n",
    "    'feature2': [4, 5, 6],\n",
    "    'target': [0, 1, 0]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Separate features and target\n",
    "X = df[['feature1', 'feature2']]\n",
    "y = df['target']\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training Features:\\n\", X_train)\n",
    "print(\"Testing Features:\\n\", X_test)\n",
    "print(\"Training Target:\\n\", y_train)\n",
    "print(\"Testing Target:\\n\", y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "271b6625-f658-4276-8caa-10a36215aa26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "    feature1  target\n",
      "7         8       1\n",
      "2         3       0\n",
      "9        10       1\n",
      "4         5       0\n",
      "3         4       1\n",
      "6         7       0\n",
      "Validation Data:\n",
      "    feature1  target\n",
      "8         9       0\n",
      "5         6       1\n",
      "Test Data:\n",
      "    feature1  target\n",
      "1         2       1\n",
      "0         1       0\n"
     ]
    }
   ],
   "source": [
    "## Q5. Perform a train-test split, dividing the data into training, validation, and test datasets.\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample data\n",
    "data = pd.DataFrame({\n",
    "    'feature1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'target': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
    "})\n",
    "\n",
    "# First, split into training and temp (validation + test)\n",
    "train_data, temp_data = train_test_split(data, test_size=0.4, random_state=42)\n",
    "\n",
    "# Now, split temp into validation and test\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"Training Data:\\n\", train_data)\n",
    "print(\"Validation Data:\\n\", val_data)\n",
    "print(\"Test Data:\\n\", test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "411e1edc-015a-4c64-ad9c-074d31269d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      " [[9 0]\n",
      " [2 1]\n",
      " [6 1]\n",
      " [1 0]\n",
      " [8 1]\n",
      " [3 0]]\n",
      "Validation Data:\n",
      " [[10  1]\n",
      " [ 5  0]]\n",
      "Test Data:\n",
      " [[4 1]\n",
      " [7 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "data = np.array([[1, 0], [2, 1], [3, 0], [4, 1], [5, 0], \n",
    "                 [6, 1], [7, 0], [8, 1], [9, 0], [10, 1]])\n",
    "\n",
    "# Shuffle the data\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Calculate split indices\n",
    "train_size = int(0.6 * len(data))\n",
    "val_size = int(0.2 * len(data))\n",
    "\n",
    "# Split the data\n",
    "train_data = data[:train_size]\n",
    "val_data = data[train_size:train_size + val_size]\n",
    "test_data = data[train_size + val_size:]\n",
    "\n",
    "print(\"Training Data:\\n\", train_data)\n",
    "print(\"Validation Data:\\n\", val_data)\n",
    "print(\"Test Data:\\n\", test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c36b8baf-1791-4746-810e-1c850e13b410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "    feature1  target\n",
      "7         8       1\n",
      "8         9       0\n",
      "1         2       0\n",
      "4         5       1\n",
      "5         6       1\n",
      "2         3       0\n",
      "Validation Data:\n",
      "    feature1  target\n",
      "3         4       1\n",
      "6         7       0\n",
      "Test Data:\n",
      "    feature1  target\n",
      "9        10       1\n",
      "0         1       0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample imbalanced data\n",
    "data = pd.DataFrame({\n",
    "    'feature1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'target': [0, 0, 0, 1, 1, 1, 0, 1, 0, 1]\n",
    "})\n",
    "\n",
    "# First, split into training and temp (validation + test) with stratification\n",
    "train_data, temp_data = train_test_split(data, test_size=0.4, stratify=data['target'], random_state=42)\n",
    "\n",
    "# Now, split temp into validation and test\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, stratify=temp_data['target'], random_state=42)\n",
    "\n",
    "print(\"Training Data:\\n\", train_data)\n",
    "print(\"Validation Data:\\n\", val_data)\n",
    "print(\"Test Data:\\n\", test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "488e6cc3-a87b-4abe-9f75-755068043f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature1  Feature2\n",
      "0      0.00      0.00\n",
      "1      0.25      0.25\n",
      "2      0.50      0.50\n",
      "3      0.75      0.75\n",
      "4      1.00      1.00\n"
     ]
    }
   ],
   "source": [
    "### Q6. Scale the dataset using an appropriate scaling technique.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Sample dataset\n",
    "data = {'Feature1': [10, 20, 30, 40, 50],\n",
    "        'Feature2': [100, 200, 300, 400, 500]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n",
    "print(scaled_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e26c6825-3a8c-4e9d-9e77-b76e3a62f05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature1  Feature2\n",
      "0 -1.414214 -1.414214\n",
      "1 -0.707107 -0.707107\n",
      "2  0.000000  0.000000\n",
      "3  0.707107  0.707107\n",
      "4  1.414214  1.414214\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample dataset\n",
    "data = {'Feature1': [10, 20, 30, 40, 50],\n",
    "        'Feature2': [100, 200, 300, 400, 500]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n",
    "print(scaled_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3df11a2e-a38c-4fb7-86fb-419e72218be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature1  Feature2\n",
      "0      -1.0      -1.0\n",
      "1      -0.5      -0.5\n",
      "2       0.0       0.0\n",
      "3       0.5       0.5\n",
      "4      48.5       1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Sample dataset with outliers\n",
    "data = {'Feature1': [10, 20, 30, 40, 1000],\n",
    "        'Feature2': [100, 200, 300, 400, 500]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize the RobustScaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n",
    "print(scaled_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "820d4d38-2ab0-4b70-94a6-54d52b3ce45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.5296 - loss: 0.6972\n",
      "Epoch 2/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5409 - loss: 0.6912\n",
      "Epoch 3/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5469 - loss: 0.6901\n",
      "Epoch 4/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5494 - loss: 0.6859\n",
      "Epoch 5/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5519 - loss: 0.6867\n",
      "Epoch 6/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5674 - loss: 0.6841\n",
      "Epoch 7/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5660 - loss: 0.6794 \n",
      "Epoch 8/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5778 - loss: 0.6805 \n",
      "Epoch 9/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5287 - loss: 0.6833 \n",
      "Epoch 10/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5877 - loss: 0.6754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1d1d73f5130>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Q7. Design and implement at least two hidden layers and an output layer for the binary categoricalvariables.\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Generate dummy data\n",
    "X = np.random.rand(1000, 20)  # 1000 samples, 20 features\n",
    "y = np.random.randint(2, size=(1000, 1))  # Binary target variable\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=20))  # First hidden layer\n",
    "model.add(Dense(32, activation='relu'))  # Second hidden layer\n",
    "model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, y, epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9cc9e488-268d-45f3-ad7d-7de3b9bddec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of sigmoid\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Input data\n",
    "X = np.random.rand(1000, 20)  # 1000 samples, 20 features\n",
    "y = np.random.randint(2, size=(1000, 1))  # Binary target variable\n",
    "\n",
    "# Initialize weights\n",
    "input_layer_neurons = 20\n",
    "hidden_layer1_neurons = 64\n",
    "hidden_layer2_neurons = 32\n",
    "output_neurons = 1\n",
    "\n",
    "weights_input_hidden1 = np.random.rand(input_layer_neurons, hidden_layer1_neurons)\n",
    "weights_hidden1_hidden2 = np.random.rand(hidden_layer1_neurons, hidden_layer2_neurons)\n",
    "weights_hidden2_output = np.random.rand(hidden_layer2_neurons, output_neurons)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(10000):\n",
    "    # Forward propagation\n",
    "    hidden_layer1_output = sigmoid(np.dot(X, weights_input_hidden1))\n",
    "    hidden_layer2_output = sigmoid(np.dot(hidden_layer1_output, weights_hidden1_hidden2))\n",
    "    output = sigmoid(np.dot(hidden_layer2_output, weights_hidden2_output))\n",
    "\n",
    "    # Backpropagation\n",
    "    output_error = y - output\n",
    "    output_delta = output_error * sigmoid_derivative(output)\n",
    "\n",
    "    hidden_layer2_error = output_delta.dot(weights_hidden2_output.T)\n",
    "    hidden_layer2_delta = hidden_layer2_error * sigmoid_derivative(hidden_layer2_output)\n",
    "\n",
    "    hidden_layer1_error = hidden_layer2_delta.dot(weights_hidden1_hidden2.T)\n",
    "    hidden_layer1_delta = hidden_layer1_error * sigmoid_derivative(hidden_layer1_output)\n",
    "\n",
    "    # Update weights\n",
    "    weights_hidden2_output += hidden_layer2_output.T.dot(output_delta)\n",
    "    weights_hidden1_hidden2 += hidden_layer1_output.T.dot(hidden_layer2_delta)\n",
    "    weights_input_hidden1 += X.T.dot(hidden_layer1_delta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1ff17279-66ea-4332-9d91-286484acea81",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the neural network architecture\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.hidden1 = nn.Linear(20, 64)  # First hidden layer\n",
    "        self.hidden2 = nn.Linear(64, 32)   # Second hidden layer\n",
    "        self.output = nn.Linear(32, 1)     # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.hidden1(x))  # Activation for first hidden layer\n",
    "        x = torch.relu(self.hidden2(x))  # Activation for second hidden layer\n",
    "        x = torch.sigmoid(self.output(x)) # Sigmoid for output layer\n",
    "        return x\n",
    "\n",
    "# Create the model\n",
    "model = NeuralNetwork()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Dummy data\n",
    "X = torch.rand(1000, 20)  # 1000 samples, 20 features\n",
    "y = torch.randint(0, 2, (1000, 1)).float()  # Binary target variable\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c1ebfb2b-c73c-449b-8865-bec671f57c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "## Q8. Create a Sequential model in Keras and add the previously designed layers to it.\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "# Initialize the Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers to the model\n",
    "model.add(Dense(64, input_dim=10))  # Input layer with 10 features\n",
    "model.add(Activation('relu'))        # Activation layer\n",
    "model.add(Dense(1))                  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3560fbc8-6689-4a8b-9dc1-75445d077712",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "# Initialize the Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers to the model\n",
    "model.add(Dense(128, input_dim=20, activation='relu'))  # First hidden layer\n",
    "model.add(Dropout(0.5))                                  # Dropout layer to prevent overfitting\n",
    "model.add(Dense(64, activation='relu'))                  # Second hidden layer\n",
    "model.add(Dense(1, activation='sigmoid'))                # Output layer for binary classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1e7b01b8-d6a0-4840-8e40-78b8952cb8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "\n",
    "# Initialize the Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers to the model\n",
    "model.add(Dense(256, input_dim=30, activation='relu'))  # First hidden layer\n",
    "model.add(BatchNormalization())                          # Batch normalization layer\n",
    "model.add(Dense(128, activation='relu'))                # Second hidden layer\n",
    "model.add(Dense(10, activation='softmax'))              # Output layer for multi-class classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "318ca210-6717-44b4-9fef-3fb05eadbe46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m2,112\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │             \u001b[38;5;34m650\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,762</span> (10.79 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,762\u001b[0m (10.79 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,762</span> (10.79 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,762\u001b[0m (10.79 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###Q9. Print the summary of the model architecture.\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Create a simple model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(32,)))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3eb7426d-7d88-4b39-877f-7188d42f807a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\n",
      "Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f61f8f47-ebe6-4654-b29a-7533ff5471a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchsummary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "\n",
    "# Define a simple model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(32, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# Create the model\n",
    "model = SimpleModel()\n",
    "\n",
    "# Print the model summary\n",
    "summary(model, (32,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6b3fcee6-ea41-4e5d-a58c-b4a9de454a88",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Add layers to the model\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m10\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[43minput_dim\u001b[49m,)))\n\u001b[0;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'input_dim' is not defined"
     ]
    }
   ],
   "source": [
    "## Q10. Set the loss function(‘binary_crossentropy’), optimizer, and include the accuracy metric in the model.\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers to the model\n",
    "model.add(Dense(10, activation='relu', input_shape=(input_dim,)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(\"Model compiled successfully with binary_crossentropy loss and accuracy metric!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fb12d0-d351-48c6-a680-8ca2442cd8cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
